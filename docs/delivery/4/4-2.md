# [4-2] Implement Speech Recognition integration

[Back to task list](./tasks.md)

## Description

Integrate Web Speech API for continuous voice input with real-time transcription. Create a custom hook that handles speech recognition initialization, start/stop controls, interim results, and error handling with browser compatibility checks.

## Status History

| Timestamp | Event Type | From Status | To Status | Details | User |
|-----------|------------|-------------|-----------|---------|------|
| 2025-10-12 00:00:00 | Created | N/A | Proposed | Task file created | Sean |

## Requirements

**Current State:** API client exists and can send questions. PushToTalk component exists with onPress/onRelease callbacks but no actual speech recognition.

1. Create `useSpeechRecognition` hook:
   - Initialize SpeechRecognition (with webkit fallback)
   - Handle browser compatibility detection
   - Continuous recognition without automatic cutoff
   - Real-time interim and final results
   - Error handling for permissions and API errors

2. Update PushToTalk component:
   - Integrate speech recognition hook
   - Start recognition on press
   - Stop recognition on release
   - Display real-time transcript
   - Show error states visually

3. Handle edge cases:
   - Permission denied
   - Browser not supported
   - Network errors
   - Recognition timeout

## Implementation Plan

###  1. Create useSpeechRecognition Hook
```typescript
// web/hooks/useSpeechRecognition.ts
import { useState, useEffect, useCallback, useRef } from 'react';

interface UseSpeechRecognitionReturn {
  isSupported: boolean;
  isListening: boolean;
  transcript: string;
  interimTranscript: string;
  error: string | null;
  startListening: () => void;
  stopListening: () => void;
  resetTranscript: () => void;
}

export function useSpeechRecognition(): UseSpeechRecognitionReturn {
  const [isSupported, setIsSupported] = useState(false);
  const [isListening, setIsListening] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [interimTranscript, setInterimTranscript] = useState('');
  const [error, setError] = useState<string | null>(null);
  
  const recognitionRef = useRef<SpeechRecognition | null>(null);

  useEffect(() => {
    // Check for browser support
    if (typeof window !== 'undefined') {
      const SpeechRecognitionAPI = 
        window.SpeechRecognition || window.webkitSpeechRecognition;
      
      if (SpeechRecognitionAPI) {
        setIsSupported(true);
        const recognition = new SpeechRecognitionAPI();
        
        // Configuration
        recognition.continuous = true;
        recognition.interimResults = true;
        recognition.lang = 'en-US';
        recognition.maxAlternatives = 1;

        // Event handlers
        recognition.onresult = (event: SpeechRecognitionEvent) => {
          let interimText = '';
          let finalText = '';

          for (let i = event.resultIndex; i < event.results.length; i++) {
            const result = event.results[i];
            const text = result[0].transcript;

            if (result.isFinal) {
              finalText += text + ' ';
            } else {
              interimText += text;
            }
          }

          if (finalText) {
            setTranscript(prev => prev + finalText);
          }
          setInterimTranscript(interimText);
        };

        recognition.onerror = (event: SpeechRecognitionErrorEvent) => {
          console.error('Speech recognition error:', event.error);
          setError(event.error);
          setIsListening(false);
        };

        recognition.onend = () => {
          setIsListening(false);
          setInterimTranscript('');
        };

        recognitionRef.current = recognition;
      }
    }

    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.stop();
      }
    };
  }, []);

  const startListening = useCallback(() => {
    if (!recognitionRef.current || isListening) return;

    try {
      setError(null);
      recognitionRef.current.start();
      setIsListening(true);
    } catch (err) {
      console.error('Failed to start recognition:', err);
      setError('Failed to start speech recognition');
    }
  }, [isListening]);

  const stopListening = useCallback(() => {
    if (!recognitionRef.current || !isListening) return;

    try {
      recognitionRef.current.stop();
    } catch (err) {
      console.error('Failed to stop recognition:', err);
    }
  }, [isListening]);

  const resetTranscript = useCallback(() => {
    setTranscript('');
    setInterimTranscript('');
    setError(null);
  }, []);

  return {
    isSupported,
    isListening,
    transcript,
    interimTranscript,
    error,
    startListening,
    stopListening,
    resetTranscript,
  };
}
```

### 2. Update page.tsx to Use Speech Recognition
```typescript
// web/app/page.tsx
import { useSpeechRecognition } from "@/hooks/useSpeechRecognition";

export default function Home() {
  const {
    isSupported,
    isListening,
    transcript,
    interimTranscript,
    error: speechError,
    startListening,
    stopListening,
    resetTranscript,
  } = useSpeechRecognition();

  const handlePressToTalk = () => {
    resetTranscript();
    startListening();
  };

  const handleReleaseToTalk = async () => {
    stopListening();
    
    if (!sessionId || !transcript) return;

    try {
      const answer = await apiClient.ask(sessionId, transcript);
      setResponse(answer);
    } catch (err) {
      console.error('Failed to ask question:', err);
      setError(err instanceof Error ? err.message : 'Failed to get response');
    }
  };

  // Display combined transcript
  const displayTranscript = transcript + interimTranscript;

  return (
    // ... render displayTranscript and speechError
  );
}
```

### 3. Create Unsupported Browser Fallback Component
```typescript
// web/components/SpeechUnsupported.tsx
export function SpeechUnsupported() {
  return (
    <Card className="border-yellow-500">
      <CardContent className="pt-6">
        <div className="flex items-start gap-3">
          <AlertCircle className="h-5 w-5 text-yellow-500" />
          <div>
            <h3 className="font-semibold">Speech Recognition Not Supported</h3>
            <p className="text-sm text-muted-foreground mt-1">
              Your browser doesn't support speech recognition. 
              Please use Chrome, Edge, or Safari on iOS.
            </p>
          </div>
        </div>
      </CardContent>
    </Card>
  );
}
```

## Test Plan

### Success Criteria

- [ ] useSpeechRecognition hook correctly detects browser support
- [ ] Hook returns isSupported as false in unsupported browsers
- [ ] startListening() initializes recognition and sets isListening to true
- [ ] Interim transcript updates in real-time as user speaks
- [ ] Final transcript is appended to full transcript
- [ ] stopListening() stops recognition and clears interim transcript
- [ ] resetTranscript() clears all transcript state
- [ ] Permission denied error is caught and displayed
- [ ] Network errors are handled gracefully
- [ ] Component shows fallback UI when speech not supported
- [ ] Transcript displays in real-time while holding button
- [ ] Question is sent with final transcript when button released

### Manual Testing

1. **Chrome/Edge (Desktop)**:
   - Hold button and speak
   - Verify interim results appear in real-time
   - Release button and verify final transcript
   - Verify question is sent to backend

2. **iOS Safari**:
   - Same flow as above
   - Verify webkit prefix works
   - Test microphone permission flow

3. **Firefox/Unsupported**:
   - Verify fallback UI appears
   - Verify button is disabled with appropriate message

## Verification

```bash
cd /home/sean/workspace/janus/web
pnpm dev

# Manual testing required:
# 1. Open http://localhost:3001 in Chrome
# 2. Click and hold push-to-talk button
# 3. Speak a question
# 4. Verify interim transcript appears
# 5. Release button
# 6. Verify final transcript is complete
# 7. Verify question is sent to backend

# Test error handling:
# 1. Deny microphone permission
# 2. Verify error message appears
# 3. Test in unsupported browser (Firefox)
# 4. Verify fallback UI appears
```

## Files Modified

### New Files
- `web/hooks/useSpeechRecognition.ts` - Custom hook for speech recognition
- `web/components/SpeechUnsupported.tsx` - Fallback component for unsupported browsers

### Modified Files
- `web/app/page.tsx` - Integrate speech recognition hook
- `web/components/PushToTalk.tsx` - Update to show listening state
- `web/lib/types.ts` - Add speech recognition types if needed


