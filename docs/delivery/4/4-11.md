# [4-11] Implement server-side speech transcription with Whisper

[Back to task list](./tasks.md)

## Description

Replace client-side Web Speech API recognition with server-side Whisper transcription for improved reliability, mobile compatibility, and transcription quality. This architectural change enables audio recording in non-HTTPS contexts and provides consistent behavior across browsers.

## Status History

| Timestamp | Event Type | From Status | To Status | Details | User |
|-----------|------------|-------------|-----------|---------|------|
| 2025-10-19 00:00:00 | Created | N/A | Proposed | Task file created to document server-side transcription work | AI_Agent |
| 2025-10-19 00:01:00 | Status Update | Proposed | InProgress | Implementation started: Whisper backend handler, MediaRecorder frontend, CORS configuration | AI_Agent |
| 2025-10-19 12:00:00 | Status Update | InProgress | Review | Implementation complete: backend transcription handler with timeout, frontend audio recorder with server upload, UUID polyfill, full integration, CodeRabbit review fixes applied | AI_Agent |
| 2025-10-19 12:30:00 | Status Update | Review | Done | Task completed and approved: server-side transcription working, all files modified documented, tests verified | Sean |

## Requirements

**Current State:** Frontend uses Web Speech API (webkitSpeechRecognition) for client-side transcription. Works only in HTTPS contexts, limited to specific browsers.

**Goal State:** Use MediaRecorder to capture audio, upload to backend, transcribe with Whisper, return text to frontend.

1. **Backend Transcription Endpoint:**
   - Accept audio file uploads (multipart/form-data)
   - Save audio to temp directory
   - Execute Whisper CLI for transcription
   - Return transcribed text as JSON
   - Clean up temp files after processing

2. **Whisper Configuration:**
   - Configurable Whisper executable path
   - Configurable model selection (base, small, medium, large)
   - Environment variable overrides
   - Proper error handling for missing Whisper installation

3. **Frontend Audio Recording:**
   - Use MediaRecorder API instead of SpeechRecognition
   - Record in WebM format with opus codec
   - Configure for 16kHz mono audio (Whisper optimal)
   - Upload audio blob to transcription endpoint
   - Handle recording, transcribing, and error states

4. **CORS Configuration:**
   - Enable CORS for mobile access via Tailscale
   - Allow all origins in development
   - Configurable via environment variables

5. **Mobile Compatibility:**
   - Works in non-HTTPS contexts (HTTP over Tailscale)
   - No browser-specific prefixes required
   - Consistent behavior across iOS Safari, Chrome, Firefox

## Implementation Plan

### 1. Backend Transcription Handler

```go
// api/internal/api/handlers/transcribe.go
package handlers

import (
	"fmt"
	"io"
	"net/http"
	"os"
	"os/exec"
	"path/filepath"
	"strings"
	"time"

	"github.com/gin-gonic/gin"
	"github.com/sean/janus/internal/config"
	"github.com/sean/janus/internal/logger"
)

type TranscribeHandler struct {
	config *config.Config
}

func NewTranscribeHandler(cfg *config.Config) *TranscribeHandler {
	return &TranscribeHandler{config: cfg}
}

type TranscribeResponse struct {
	Text string `json:"text"`
}

func (h *TranscribeHandler) Transcribe(c *gin.Context) {
	log := logger.Get()

	// Get audio file from multipart form
	file, header, err := c.Request.FormFile("audio")
	if err != nil {
		log.Error().Err(err).Msg("Failed to get audio file")
		c.JSON(http.StatusBadRequest, gin.H{"error": "No audio file provided"})
		return
	}
	defer file.Close()

	log.Info().
		Str("filename", header.Filename).
		Int64("size", header.Size).
		Msg("Received audio file for transcription")

	// Create temp directory
	tempDir := filepath.Join(os.TempDir(), "janus-transcribe")
	if err := os.MkdirAll(tempDir, 0755); err != nil {
		log.Error().Err(err).Msg("Failed to create temp directory")
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Internal server error"})
		return
	}

	// Save audio file
	timestamp := time.Now().UnixNano()
	audioExt := filepath.Ext(header.Filename)
	if audioExt == "" {
		audioExt = ".webm"
	}
	audioPath := filepath.Join(tempDir, fmt.Sprintf("audio_%d%s", timestamp, audioExt))

	audioFile, err := os.Create(audioPath)
	if err != nil {
		log.Error().Err(err).Msg("Failed to create audio file")
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Internal server error"})
		return
	}

	if _, err := io.Copy(audioFile, file); err != nil {
		audioFile.Close()
		os.Remove(audioPath)
		log.Error().Err(err).Msg("Failed to save audio file")
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Internal server error"})
		return
	}
	audioFile.Close()
	defer os.Remove(audioPath)

	// Run Whisper transcription
	text, err := h.runWhisper(audioPath)
	if err != nil {
		log.Error().Err(err).Msg("Whisper transcription failed")
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Transcription failed"})
		return
	}

	log.Info().Str("text", text).Msg("Transcription successful")

	c.JSON(http.StatusOK, TranscribeResponse{Text: text})
}

func (h *TranscribeHandler) runWhisper(audioPath string) (string, error) {
	log := logger.Get()
	outputDir := filepath.Dir(audioPath)

	cmd := exec.Command(
		h.config.WhisperPath,
		audioPath,
		"--model", h.config.WhisperModel,
		"--output_format", "txt",
		"--output_dir", outputDir,
	)

	log.Debug().
		Str("whisper_path", h.config.WhisperPath).
		Str("audio_path", audioPath).
		Str("model", h.config.WhisperModel).
		Msg("Executing whisper command")

	output, err := cmd.CombinedOutput()
	if err != nil {
		log.Error().
			Err(err).
			Str("output", string(output)).
			Msg("Whisper command failed")
		return "", fmt.Errorf("whisper command failed: %w", err)
	}

	// Read transcription file
	baseName := strings.TrimSuffix(filepath.Base(audioPath), filepath.Ext(audioPath))
	txtPath := filepath.Join(outputDir, baseName+".txt")
	defer os.Remove(txtPath)

	textBytes, err := os.ReadFile(txtPath)
	if err != nil {
		log.Error().Err(err).Msg("Failed to read transcription file")
		return "", fmt.Errorf("failed to read transcription: %w", err)
	}

	return strings.TrimSpace(string(textBytes)), nil
}
```

### 2. Configuration Updates

```go
// api/internal/config/config.go

type Config struct {
	// ... existing fields ...
	WhisperPath  string
	WhisperModel string
}

const (
	// ... existing constants ...
	DefaultWhisperPath  = "/home/sean/whisper-local/.venv/bin/whisper"
	DefaultWhisperModel = "base"
)

func Load() (*Config, error) {
	config := &Config{
		// ... existing fields ...
		WhisperPath:  getEnv("WHISPER_PATH", DefaultWhisperPath),
		WhisperModel: getEnv("WHISPER_MODEL", DefaultWhisperModel),
	}
	// ... validation ...
}
```

### 3. Router Registration

```go
// api/internal/api/router.go

func SetupRouter(cfg *config.Config, sessionManager session.Manager) *gin.Engine {
	// ... existing setup ...
	
	transcribeHandler := handlers.NewTranscribeHandler(cfg)
	
	api := router.Group("/api")
	{
		// ... existing routes ...
		
		// Speech-to-text
		api.POST("/transcribe", transcribeHandler.Transcribe)
	}
	
	return router
}
```

### 4. CORS Configuration

```go
// api/internal/api/middleware/cors.go

const (
	DefaultCORSAllowedOrigins = "*" // Allow all for development
)
```

### 5. Frontend Audio Recorder Hook

```typescript
// web/hooks/useAudioRecorder.ts

import { useState, useEffect, useRef, useCallback } from 'react';
import { apiClient } from '@/lib/api-client';

interface UseAudioRecorderReturn {
  isSupported: boolean;
  isRecording: boolean;
  isTranscribing: boolean;
  error: string | null;
  startRecording: () => void;
  stopRecording: () => Promise<string>;
  resetRecording: () => void;
}

export function useAudioRecorder(): UseAudioRecorderReturn {
  const [isSupported, setIsSupported] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [isTranscribing, setIsTranscribing] = useState(false);
  const [error, setError] = useState<string | null>(null);

  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const streamRef = useRef<MediaStream | null>(null);

  // Check browser support
  useEffect(() => {
    if (typeof window !== 'undefined' && 
        navigator.mediaDevices && 
        navigator.mediaDevices.getUserMedia) {
      setIsSupported(true);
    }
  }, []);

  const startRecording = useCallback(async () => {
    if (isRecording) return;

    try {
      setError(null);
      audioChunksRef.current = [];

      // Request microphone access
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          channelCount: 1,
          sampleRate: 16000, // Whisper prefers 16kHz
        } 
      });
      streamRef.current = stream;

      // Create MediaRecorder
      const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus')
        ? 'audio/webm;codecs=opus'
        : 'audio/webm';

      const mediaRecorder = new MediaRecorder(stream, { mimeType });
      mediaRecorderRef.current = mediaRecorder;

      // Collect audio chunks
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      mediaRecorder.onerror = (event) => {
        console.error('MediaRecorder error:', event);
        setError('Recording error occurred');
        setIsRecording(false);
      };

      // Start recording
      mediaRecorder.start(100); // Collect data every 100ms
      setIsRecording(true);
    } catch (err) {
      console.error('Failed to start recording:', err);
      const errorMessage = err instanceof Error 
        ? err.message 
        : 'Failed to start audio recording. Please allow microphone access.';
      setError(errorMessage);
    }
  }, [isRecording]);

  const stopRecording = useCallback(async (): Promise<string> => {
    if (!mediaRecorderRef.current || !isRecording) {
      return '';
    }

    return new Promise((resolve, reject) => {
      const mediaRecorder = mediaRecorderRef.current!;

      mediaRecorder.onstop = async () => {
        // Stop all tracks
        if (streamRef.current) {
          streamRef.current.getTracks().forEach(track => track.stop());
          streamRef.current = null;
        }

        setIsRecording(false);

        // Create audio blob
        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });

        // Transcribe the audio
        try {
          setIsTranscribing(true);
          const transcription = await apiClient.transcribe(audioBlob);
          setIsTranscribing(false);
          setError(null);
          resolve(transcription);
        } catch (err) {
          console.error('Transcription failed:', err);
          setIsTranscribing(false);
          const errorMessage = err instanceof Error 
            ? err.message 
            : 'Failed to transcribe audio';
          setError(errorMessage);
          reject(new Error(errorMessage));
        }
      };

      mediaRecorder.stop();
    });
  }, [isRecording]);

  const resetRecording = useCallback(() => {
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop();
    }
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    audioChunksRef.current = [];
    setIsRecording(false);
    setIsTranscribing(false);
    setError(null);
  }, [isRecording]);

  return {
    isSupported,
    isRecording,
    isTranscribing,
    error,
    startRecording,
    stopRecording,
    resetRecording,
  };
}
```

### 6. API Client Update

```typescript
// web/lib/api-client.ts

class CursorVoiceClient {
  // ... existing methods ...

  async transcribe(audioBlob: Blob): Promise<string> {
    const formData = new FormData();
    formData.append('audio', audioBlob, 'recording.webm');

    const response = await fetch(`${this.baseUrl}/transcribe`, {
      method: 'POST',
      body: formData,
    });

    if (!response.ok) {
      throw new Error(`Transcription failed: ${response.statusText}`);
    }

    const data = await response.json();
    return data.text;
  }
}
```

### 7. Frontend Integration

Update `web/app/page.tsx` to use `useAudioRecorder` instead of `useSpeechRecognition`:

```typescript
// Replace useSpeechRecognition with useAudioRecorder
const {
  isSupported,
  isRecording,
  isTranscribing,
  error: recordingError,
  startRecording: startAudioRecording,
  stopRecording: stopAudioRecording,
  resetRecording,
} = useAudioRecorder();

// Update state management to include transcribing state
useEffect(() => {
  if (isSpeaking || isGeneratingAudio) {
    setState("speaking");
  } else if (isRecording) {
    setState("recording");
  } else if (isTranscribing) {
    setState("processing");
  } else if (state === "speaking" || state === "recording" || state === "processing") {
    setState("idle");
  }
}, [isSpeaking, isGeneratingAudio, isRecording, isTranscribing, state]);

// Update recording handlers
const stopRecording = useCallback(async () => {
  try {
    setState("processing");
    const transcribedText = await stopAudioRecording();
    
    if (!transcribedText) {
      setState("idle");
      return;
    }

    await processQuestion(transcribedText);
  } catch (err) {
    console.error("Failed to process recording:", err);
    setError(err instanceof Error ? err.message : "Failed to process recording");
    setState("error");
  }
}, [stopAudioRecording, processQuestion]);
```

## Architecture Change

### Before: Client-Side Recognition
```
┌─────────────┐    ┌──────────────────┐    ┌─────────────┐
│   Browser   │───▶│ Web Speech API   │───▶│   Backend   │
│ (SpeechRec) │    │ (Real-time text) │    │ (Ask Q&A)   │
└─────────────┘    └──────────────────┘    └─────────────┘
```

**Limitations:**
- Requires HTTPS (no HTTP over Tailscale)
- Browser-specific (webkit prefix required)
- No real-time transcript without HTTPS
- Inconsistent behavior across browsers

### After: Server-Side Transcription
```
┌─────────────┐    ┌───────────────┐    ┌─────────────┐    ┌─────────────┐
│   Browser   │───▶│ MediaRecorder │───▶│   Backend   │───▶│   Whisper   │
│  (Record)   │    │ (Audio blob)  │    │ (Transcribe)│    │   (Text)    │
└─────────────┘    └───────────────┘    └─────────────┘    └─────────────┘
                                              │
                                              ▼
                                        ┌─────────────┐
                                        │   Backend   │
                                        │ (Ask Q&A)   │
                                        └─────────────┘
```

**Benefits:**
- Works in HTTP contexts (Tailscale)
- No browser-specific code
- State-of-the-art transcription quality
- Consistent behavior everywhere

**Trade-offs:**
- No real-time transcript during recording
- Additional network round-trip
- Requires Whisper server-side
- Larger data transfer (audio vs text)

## Test Plan

### Backend Tests

1. **Transcription Endpoint**
   ```bash
   curl -X POST http://localhost:3000/api/transcribe \
     -F "audio=@test-audio.webm"
   ```
   - Should return `{"text": "transcribed text"}`
   - Audio file should be cleaned up
   - Whisper should be called with correct parameters

2. **Error Handling**
   - Missing audio file → 400 Bad Request
   - Whisper not installed → 500 Internal Server Error
   - Invalid audio format → Should still attempt transcription

3. **Configuration**
   ```bash
   WHISPER_PATH=/custom/path/whisper \
   WHISPER_MODEL=small \
   go run cmd/server/main.go
   ```
   - Should use custom Whisper path and model

### Frontend Tests

1. **Audio Recording**
   - Click to start recording
   - Verify `isRecording` becomes true
   - Speak for 5 seconds
   - Click to stop
   - Verify state transitions: recording → processing → idle

2. **Transcription**
   - Record "What is the main function?"
   - Verify transcription is returned
   - Verify question is sent to backend
   - Verify response is received

3. **Error Handling**
   - Deny microphone permission
   - Verify error message displayed
   - Backend down
   - Verify transcription failure handled

4. **Mobile Compatibility**
   - Test on iOS Safari (HTTP over Tailscale)
   - Test on Android Chrome
   - Verify recording works in both
   - Verify transcription quality

### Integration Tests

1. **Complete Flow**
   - Start recording
   - Speak question
   - Stop recording
   - Verify transcription
   - Verify question sent
   - Verify response received
   - Verify TTS playback

2. **Performance**
   - Record 10 second audio
   - Measure transcription time
   - Should complete within 5 seconds for base model

## Verification

```bash
# Backend
cd /home/sean/workspace/janus/api
go build ./cmd/server
./server

# Frontend
cd /home/sean/workspace/janus/web
pnpm dev

# Test transcription
# 1. Open http://localhost:3001
# 2. Click anywhere to start recording
# 3. Speak a question
# 4. Release to stop
# 5. Verify "Processing..." state
# 6. Verify transcription appears in conversation
```

## Files Modified

### New Files (Backend)
- `api/internal/api/handlers/transcribe.go` - Complete Whisper transcription handler with temp file management and cleanup

### Modified Files (Backend)
- `api/internal/api/router.go` - Added `POST /api/transcribe` endpoint registration
- `api/internal/config/config.go` - Added WhisperPath and WhisperModel configuration with defaults and env var support
- `api/internal/api/middleware/cors.go` - Updated CORS to allow all origins (*) for mobile development

### New Files (Frontend)
- `web/hooks/useAudioRecorder.ts` - MediaRecorder-based audio recording hook with server-side transcription
- `web/lib/uuid.ts` - UUID polyfill for Safari without crypto.randomUUID() support

### Modified Files (Frontend)
- `web/app/page.tsx` - Replaced useSpeechRecognition with useAudioRecorder, added processing state, improved error handling
- `web/lib/api-client.ts` - Added transcribe() method for audio upload
- `web/components/state-indicator.tsx` - Added "processing" state for transcription
- `web/components/conversation-history.tsx` - Removed live transcript display (server-side only)
- `web/package.json` - Added eruda for mobile debugging

## Dependencies

### Runtime Dependencies
- **Whisper:** Must be installed at configured path
  ```bash
  pip install openai-whisper
  # or
  pip install git+https://github.com/openai/whisper.git
  ```

- **Whisper Models:** Downloaded automatically on first use
  - base: ~150MB, fastest
  - small: ~500MB, good quality
  - medium: ~1.5GB, better quality
  - large: ~3GB, best quality

### Environment Variables
```bash
# Optional, defaults provided
WHISPER_PATH=/path/to/whisper
WHISPER_MODEL=base
CORS_ALLOWED_ORIGINS=*
```

### Disk Space
- Temp audio files: ~1-5MB per request
- Temp transcription files: <1KB per request
- All cleaned up immediately after processing

## Notes

- This represents a significant architectural change from client-side to server-side transcription
- Improves mobile compatibility and removes HTTPS requirement for voice input
- Trade-off: No real-time transcript, but better quality and reliability
- Whisper base model provides good balance of speed and accuracy
- Can upgrade to small/medium model for better quality at cost of speed

